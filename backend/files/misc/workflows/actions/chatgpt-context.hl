
/*
 * Invokes ChatGPT with the specified [prompt], [model], [max_tokens], [instruction] and [context].
 *
 * This action will use the default API key found from configurations, and the specified [context]
 * and [instruction] to answer the question specified as [prompt]. This allows you to answer a question
 * by using a context found by e.g. using the [context-from-database] action or the [context-from-search]
 * action, and an instruction saying for instance "Answer my question exclusively using the specified context",
 * which then will invoke ChatGPT with your own custom data, having ChatGPT exclusively using your data
 * to answer your questions.
 */
.arguments
   model
      type:enum
      mandatory:bool:true
      default:gpt-4-1106-preview
      values
         .:gpt-3.5-turbo
         .:gpt-3.5-turbo-16k
         .:gpt-4
         .:gpt-4-1106-preview
   max_tokens
      type:int
      mandatory:bool:true
      default:int:1500
   instruction
      type:textarea
      mandatory:bool:true
      default:Answer my question using the information found in the context
   context
      type:textarea
      mandatory:bool:true
   prompt
      type:textarea
      mandatory:bool:true
.icon:chat_bubble

// Sanity checking invocation.
validators.mandatory:x:@.arguments/*/model
validators.mandatory:x:@.arguments/*/prompt
validators.mandatory:x:@.arguments/*/max_tokens

// Retrieving OpenAI API token from configuration settings.
.token
set-value:x:@.token
   strings.concat
      .:"Bearer "
      config.get:"magic:openai:key"

// Invokes OpenAI.
http.post:"https://api.openai.com/v1/chat/completions"
   convert:bool:true
   headers
      Authorization:x:@.token
      Content-Type:application/json
   payload
      model:x:@.arguments/*/model
      max_tokens:x:@.arguments/*/max_tokens
      temperature:decimal:0.3
      messages
         .
            role:system
            content:x:@.arguments/*/instruction
         .
            role:user
            content:x:@.arguments/*/context
         .
            role:user
            content:x:@.arguments/*/prompt

// Sanity checking above invocation.
if
   not
      and
         mte:x:@http.post
            .:int:200
         lt:x:@http.post
            .:int:300
   .lambda

      // Oops, error - Logging error and returning status 500 to caller.
      lambda2hyper:x:@http.post
      log.error:Something went wrong while invoking OpenAI
         message:x:@http.post/*/content/*/error/*/message
         status:x:@http.post
         error:x:@lambda2hyper
      throw:Something went wrong while invoking OpenAI
         message:x:@http.post/*/content/*/error/*/message
         status:x:@http.post
         error:x:@lambda2hyper

// Returning result to caller.
yield
   result:x:@http.post/*/content/*/choices/0/*/message/*/content


// OpenAI chat endpoint, for having conversations using "gpt-xxx" models.
.arguments
   prompt:string
   type:string
   references:bool
   chat:bool
   recaptcha_response:string
   user_id:string
   session:string
.description:@"OpenAI chat endpoint, for having conversations using 'gpt-xxx' models"
.type:public

// Invoking slot containing commonalities for all endpoints.
insert-before:x:../*/signal/*/.callback
   get-nodes:x:@.arguments/*
signal:magic.ai.endpoint-common
   .callback

      // Checking if prompt contains an email address.
      if
         and
            strings.contains:x:@.arguments/*/prompt
               .:@
            not-null:x:@.arguments/*/contact_us
            neq:x:@.arguments/*/contact_us
               .:
            not-null:x:@.arguments/*/lead_email
            neq:x:@.arguments/*/lead_email
               .:
         .lambda

            /*
             * Prompt contains an email address, and model is configured with a
             * lead email address or [contact_us]/[lead_email] property - Hence
             * treating prompt as a lead if we can, sending an email to lead email
             * address with the conversation.
             */
            unwrap:x:+/*
            signal:magic.ai.send-lead-email
               type:x:@.arguments/*/type
               prompt:x:@.arguments/*/prompt
               session:x:@.arguments/*/session
               user_id:x:@.arguments/*/user_id
               lead_email:x:@.arguments/*/lead_email
               contact_us:x:@.arguments/*/contact_us
            if:x:@signal
               unwrap:x:+/*
               return
                  db_time:decimal:-1
                  finish_reason:cached
                  result:x:@.arguments/*/contact_us

      // Checking what type of model this is, and invoking correct slot accordingly.
      if
         strings.starts-with:x:@.arguments/*/model
            .:gpt
         .lambda

            // Invoking chat style of slot.
            add:x:./*/signal
               get-nodes:x:@.arguments/*
            signal:magic.ai.chat
            return-nodes:x:@signal/*

      else

         // Invoking chat style of slot.
         add:x:./*/signal
            get-nodes:x:@.arguments/*
         signal:magic.ai.completion
         return-nodes:x:@signal/*

/*
 * Applying some HTTP caching to avoid invoking OpenAI again with
 * the same question before some minimum amount of time has passed.
 */
response.headers.set
   Cache-Control:max-age=30

// Returning result of worker slot to caller.
return-nodes:x:@signal/*

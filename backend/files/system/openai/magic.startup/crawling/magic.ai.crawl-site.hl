
/*
 * Crawls the specified website generating training data for machine learning in the process.
 */
slots.create:magic.ai.crawl-site

   // Creating a thread and invoking file doing the heavy lifting.
   insert-before:x:./*/fork/0
      get-nodes:x:@.arguments
   fork

      // Making sure exceptions does not leave thread.
      try

         /*
          * Loading robots.txt from specified [url].
          */
         unwrap:x:+/*
         signal:magic.ai.load-robots
            url:x:@.arguments/*/url
            feedback-channel:x:@.arguments/*/feedback-channel

         // Checking if site contains a robots.txt file.
         if
            eq:x:@signal/*/found
               .:bool:true
            .lambda

               // Site contains a robots.txt file, signaling frontend of that fact.
               sockets.signal:x:@.arguments/*/feedback-channel
                  args
                     message:Site has robots.txt
                     type:info
               sleep:100

               // Signaling frontend how many sitemaps we found in robots.txt file.
               strings.concat
                  .:"Found "
                  get-count:x:@signal/*/sitemap/*
                  .:" sitemaps in robots.txt file"
               unwrap:x:+/**
               sockets.signal:x:@.arguments/*/feedback-channel
                  args
                     message:x:@strings.concat
                     type:info
               sleep:100

               // Checking if robots.txt contains a crawl-delay.
               if
                  exists:x:@signal/*/crawl-delay
                  .lambda

                     // Updating delay to value from robots.txt.
                     remove-nodes:x:@.arguments/*/delay
                     unwrap:x:+/*
                     validators.default:x:@.arguments
                        delay:x:@signal/*/crawl-delay


                     // Signaling frontend to inform of that we found a crawl-delay value.
                     strings.concat
                        .:"Robots.txt file contains a Crawl-Delay value of "
                        math.divide:x:@signal/*/crawl-delay
                           .:int:1000
                        .:" seconds"
                     unwrap:x:+/**
                     sockets.signal:x:@.arguments/*/feedback-channel
                        args
                           message:x:@strings.concat
                           type:info

         else

            // Site does not contain a robots.txt file, signaling that fact to frontend.
            sockets.signal:x:@.arguments/*/feedback-channel
               args
                  message:Could not find a robots.txt file for website
                  type:warning
            sleep:100
            strings.concat
               .:"We will try to retrieve sitemap from "
               get-value:x:@signal/*/sitemap/0
            unwrap:x:+/**
            sockets.signal:x:@.arguments/*/feedback-channel
               args
                  message:x:@strings.concat
                  type:info
            sleep:100

         /*
          * Trying to load URLs from sitemap returned from above invocation.
          */
         add:x:./*/signal/[1,2]
            get-nodes:x:@signal/*/sitemap
            get-nodes:x:@signal/*/allow
            get-nodes:x:@signal/*/disallow
         unwrap:x:+/*
         signal:magic.ai.load-sitemap
            max:x:@.arguments/*/max
            feedback-channel:x:@.arguments/*/feedback-channel
            url:x:@.arguments/*/url

         // Verifying we found at least one sitemap.
         if
            eq:x:@signal/*/has-sitemap
               .:bool:true
            .lambda

               /*
                * We found at least one sitemap.
                *
                * Signaling frontend how many URLs we found, and how many there are in total.
                */
               get-count:x:@signal/*/urls/*
               strings.concat
                  .:"We found "
                  get-value:x:@signal/*/total
                  .:" URLs in sitemap(s), we will be scraping "
                  get-value:x:@get-count
                  .:" URLs"
               unwrap:x:+/**
               sockets.signal:x:@.arguments/*/feedback-channel
                  args
                     message:x:@strings.concat
                     type:info
               sleep:100

               // Checking if site contains more URLs than we're scraping.
               if
                  eq:x:@get-count
                     .:int:0
                  .lambda

                     // Warning user!
                     strings.concat
                        .:"Warning, we could not find a single valid URL in site, probably because sitemap or robots.txt file prohibits scraping, or because your filter is entirely excluded from robots.txt for AINIRO selector"
                     unwrap:x:+/**
                     sockets.signal:x:@.arguments/*/feedback-channel
                        args
                           message:x:@strings.concat
                           type:warning
                     sleep:100
               else-if
                  mt
                     get-value:x:@signal/*/total
                     get-value:x:@get-count
                  .lambda

                     // Warning user!
                     strings.concat
                        .:"Warning, site contains more than "
                        get-value:x:@get-count
                        .:" URLs and will only be partially scraped"
                     unwrap:x:+/**
                     sockets.signal:x:@.arguments/*/feedback-channel
                        args
                           message:x:@strings.concat
                           type:warning
                     sleep:100

               // Feedback about URLs we're about to scrape, but only if there are any URLs.
               if
                  mt:x:@get-count
                     .:int:0
                  .lambda

                     // Adding spacer.
                     sockets.signal:x:@.arguments/*/feedback-channel
                        args
                           message:------------------------------------------------------------------------------------------------------------------------
                           type:info
                     sleep:100
                        sockets.signal:x:@.arguments/*/feedback-channel
                           args
                              message:"URLs we will scrape are as follows:"
                              type:info
                        sleep:100

               // Iterating through each URL returned from above invocation.
               for-each:x:@signal/*/urls/*

                  unwrap:x:+/**
                  sockets.signal:x:@.arguments/*/feedback-channel
                     args
                        message:x:@.dp/#
                        type:info
                  sleep:100

               // Iterating through each URL returned from above invocation.
               for-each:x:@signal/*/urls/*

                  // Making sure we trap exceptions.
                  try

                     // Adding spacer.
                     sockets.signal:x:@.arguments/*/feedback-channel
                        args
                           message:------------------------------------------------------------------------------------------------------------------------
                           type:info
                     sleep:100

                     // Scraping currently iterated URL.
                     unwrap:x:+/*
                     signal:magic.ai.url.scrape
                        url:x:@.dp/#
                        type:x:@.arguments/*/type
                        images:bool:true
                        code:bool:true
                        lists:bool:true
                        main:bool:true
                        empty-completion:bool:false
                        threshold:x:@.arguments/*/threshold
                        feedback-channel:x:@.arguments/*/feedback-channel

                     // Verifying we've got more snippets before applying Crawl-Delay
                     if
                        neq:x:@.dp/#
                           get-value:x:@signal/@signal/*/urls/0/-
                        .lambda

                           // Signaling frontend that we're waiting for n seconds.
                           strings.concat
                              .:"Waiting for "
                              math.divide:x:@.arguments/*/delay
                                 .:int:1000
                              .:" seconds to avoid exhausting web server"
                           unwrap:x:+/**
                           sockets.signal:x:@.arguments/*/feedback-channel
                              args
                                 message:x:@strings.concat
                                 type:info
                           sleep:100

                           // Sleeping for [delay] milliseconds to avoid exhausting web server.
                           sleep:x:@.arguments/*/delay

                  .catch

                     // Logging as error.
                     log.error:Could not scrape URL
                        url:x:@.dp/#
                        message:x:@.arguments/*/message

                     // Signaling frontend to inform about error.
                     strings.concat
                        .:"Could not scrape URL, error was: '"
                        get-value:x:@.arguments/*/message
                        .:"'"
                     unwrap:x:+/**
                     sockets.signal:x:@.arguments/@.arguments/*/feedback-channel
                        roles:root
                        args
                           message:x:@strings.concat
                           type:warning
                     sleep:100

               // Adding spacer.
               sockets.signal:x:@.arguments/*/feedback-channel
                  args
                     message:------------------------------------------------------------------------------------------------------------------------
                     type:info
               sleep:100

               /*
                * Crawling is done.
                * Making sure we notify client that we're done and do some logging.
                */
               sockets.signal:magic.backend.message
                  roles:root
                  args
                     message:Done creating OpenAI training data from URL
                     type:success
               sleep:100

               // Basic logging.
               log.info:OpenAI training data successfully created
                  url:x:@.arguments/*/url
                  type:x:@.arguments/*/type

               // Checking if caller wants us to execute some lambda object once we're done.
               if
                  exists:x:@.arguments/*/.onafter
                  .lambda
                     eval:x:@.arguments/*/.onafter

         else

            /*
             * Site did not have a valid sitemap, hence we
             * try to crawl it manually instead.
             *
             * This is the list of URLs we should scrape.
             */
            .urls

            // This is the list of URLs we already have scraped.
            .done

            // Adding root URL to above list of URLs to be crawled.
            unwrap:x:+/*/*
            add:x:@.urls
               .
                  .:x:@.arguments/*/url

            // No sitemap(s) found, informing user
            sockets.signal:x:@.arguments/*/feedback-channel
               args
                  message:Could not find any valid sitemaps
                  type:warning
            sleep:100

            // Informing frontend of that we'll try to crawl site.
            sockets.signal:x:@.arguments/*/feedback-channel
               args
                  message:Trying to crawl site even though we did not find a valid sitemap
                  type:info
            sleep:100

            /*
             * Looping through all above [.urls] as long as we don't exceed [max] argument,
             * and for as long as we have URLs to scrape.
             */
            while
               and
                  exists:x:@.urls/*
                  lt
                     get-count:x:@.done/*
                     get-value:x:@.arguments/*/max
               .lambda

                  // Adding spacer.
                  sockets.signal:x:@.arguments/*/feedback-channel
                     args
                        message:------------------------------------------------------------------------------------------------------------------------
                        type:info
                  sleep:100

                  /*
                   * Scraping first URL in above [.urls] informing slot that
                   * we want it to return URLs found during scraping.
                   */
                  unwrap:x:+/*
                  signal:magic.ai.url.scrape
                     url:x:@.urls/0
                     type:x:@.arguments/*/type
                     images:bool:true
                     code:bool:true
                     lists:bool:true
                     main:bool:true
                     empty-completion:bool:false
                     threshold:x:@.arguments/*/threshold
                     feedback-channel:x:@.arguments/*/feedback-channel

                  /*
                   * Adding currently iterated URL to [.done] and removing it
                   * from above [.urls] collection.
                   */
                  add:x:@.done
                     get-nodes:x:@.urls/0
                  remove-nodes:x:@.urls/0

                  /*
                   * Adding all URLs returned in above invocation to above [.urls] collection,
                   * unless we've already crawled the URL.
                   */
                  for-each:x:@signal/*

                     // Checking if URL has been imported or added before.
                     if
                        and
                           not-exists:x:@.done/*/={@.dp/#}
                           not-exists:x:@.urls/*/={@.dp/#}
                        .lambda

                           // Adding URL to [.urls] collection.
                           add:x:@.urls
                              get-nodes:x:@.dp/#

                  // Signaling frontend that we're waiting for n seconds.
                  strings.concat
                     .:"Waiting for "
                     math.divide:x:@.arguments/*/delay
                        .:int:1000
                     .:" seconds to avoid exhausting web server"
                  unwrap:x:+/**
                  sockets.signal:x:@.arguments/*/feedback-channel
                     args
                        message:x:@strings.concat
                        type:info
                  sleep:100

                  // Sleeping for [delay] milliseconds to avoid exhausting web server.
                  sleep:x:@.arguments/*/delay

            // Adding spacer.
            sockets.signal:x:@.arguments/*/feedback-channel
               args
                  message:------------------------------------------------------------------------------------------------------------------------
                  type:info
            sleep:100

            // Informing frontend of that we're done crawling.
            strings.concat
               .:"Done scraping "
               get-count:x:@.done/*
               .:" URLs"
            unwrap:x:+/**
            sockets.signal:x:@.arguments/*/feedback-channel
               args
                  message:x:@strings.concat
                  type:info
            sleep:100

            // Basic logging.
            log.info:OpenAI training data successfully created
               url:x:@.arguments/*/url
               type:x:@.arguments/*/type

            // Checking if caller wants us to execute some lambda object once we're done.
            if
               exists:x:@.arguments/*/.onafter
               .lambda
                  eval:x:@.arguments/*/.onafter

      .catch

         // Oops ...!!
         log.error:x:@.arguments/*/message
            url:x:@.arguments/*/url

         // Signaling frontend.
         strings.concat
            .:"Error - "
            get-value:x:@.arguments/*/message
         unwrap:x:+/*/args/*
         sockets.signal:x:@.arguments/@.arguments/*/feedback-channel
            roles:root
            args
               message:x:@strings.concat
               type:error
         sleep:100

         // Checking if caller supplied [.onerror] handler.
         if
            exists:x:@.onerror
            .lambda
               add:x:+/+
                  get-nodes:x:@.arguments/*
               add:x:+
                  get-nodes:x:@.arguments/@.arguments/*
               invoke:x:@.onerror

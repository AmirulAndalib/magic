
/*
 * Chat slot for having conversations with OpenAI's "gpt" type of models.
 */
slots.create:magic.ai.chat

   // Retrieving relevant snippets.
   unwrap:x:+/*
   signal:magic.ai.get-context
      type:x:@.arguments/*/type
      prompt:x:@.arguments/*/prompt
      threshold:x:@.arguments/*/threshold
      max_tokens:x:@.arguments/*/max_context_tokens
      vector_model:x:@.arguments/*/vector_model
      api_key:x:@.arguments/*/api_key

   // Checking if we've got a cached result.
   if
      exists:x:@signal/*/cached
      .lambda

         // Cached result, checking if type is "supervised".
         if
            and
               exists:x:@.arguments/*/supervised
               not-null:x:@.arguments/*/supervised
               eq
                  convert:x:@.arguments/*/supervised
                     type:int
                  .:int:1
            .lambda

               // Storing response to ml_requests to keep history.
               data.connect:[generic|magic]
                  data.create
                     table:ml_requests
                     values
                        type:x:@.arguments/*/type
                        prompt:x:@.arguments/*/prompt
                        completion:x:@signal/*/cached
                        finish_reason:cached
                        session:x:@.arguments/*/session
                        user_id:x:@.arguments/*/user_id

         // Returning cached result to caller.
         unwrap:x:+/*
         return
            result:x:@signal/*/cached
            finish_reason:cached

   /*
    * Session for user containing previous questions and answers,
    * in addition to context and system message.
    */
   .session

   // Checking if we've got a system message.
   if
      and
         not-null:x:@.arguments/*/system_message
         neq:x:@.arguments/*/system_message
            .:
      .lambda

         // Adding system message to [.session]
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:system
                  content:x:@.arguments/*/system_message

   // Then adding context ml_requests belonging to user to [.session]
   data.connect:[generic|magic]

      // Selecting context requests from database.
      data.select:@"
select prompt, max(completion) as completion
   from ml_requests
   where user_id = @user_id and questionnaire = 1 and context = 1
   group by prompt"
         user_id:x:@.arguments/*/user_id

      // Adding context requests to [.session].
      for-each:x:@data.select/*

         // Adding question as system message.
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:system
                  content:x:@.dp/#/*/prompt

         // Adding answer as user message.
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:user
                  content:x:@.dp/#/*/completion

   /*
    * Storing how many fixed messages we have to avoid removing
    * system message or questionnaire messages as we're pruning messages
    * to avoid overflowing model.
    */
   .fixed
   set-value:x:@.fixed
      get-count:x:@.session/*

   // Checking if we've got a cached session.
   if
      and
         exists:x:@.arguments/*/session
         not-null:x:@.arguments/*/session
         neq:x:@.arguments/*/session
            .:
      .lambda

         // Caller provided a cache key.
         cache.get:x:@.arguments/*/session
         if
            not-null:x:@cache.get
            .lambda

               // Adding session questions to context.
               add:x:@.session
                  hyper2lambda:x:@cache.get

   // Checking if we've got context.
   if
      not-null:x:@signal/*/context
      .lambda

         // Adding context to above [.session].
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:system
                  content:x:@signal/*/context

   /*
    * Pruning previous messages until size of context + max_tokens is less than whatever
    * amount of tokens the model can handle.
    *
    * Notice, we make sure we always submit the FIRST message and the SECOND message,
    * if existing, since these are the system message, and
    * the training data supplied as our context.
    */
   .cont:bool:true
   while:x:@.cont

      lambda2hyper:x:@.session/*
      if
         lt
            math.add
               get-value:x:@.arguments/*/max_tokens
               openai.tokenize:x:@lambda2hyper
            get-value:x:@.arguments/*/model_size
         .lambda

            // Context is small enough to answer according to max_tokens.
            set-value:x:@.cont
               .:bool:false

      else

         /*
          * We need to remove one of our previous messages to be able to have OpenAI return max_tokens.
          * Notice, we only remove messages after our "fixed" messages, which are system message and
          * questionnaire messages.
          */
         strings.concat
            .:@.session/
            get-value:x:@.fixed
         set-x:x:./*/remove-nodes
            convert:x:@strings.concat
               type:x
         remove-nodes

   // Checking if model is prefixed, at which point we prepend prefix to question.
   .only-prompt
   set-value:x:@.only-prompt
      get-value:x:@.arguments/*/prompt
   if
      and
         exists:x:@.arguments/*/prefix
         not-null:x:@.arguments/*/prefix
         neq:x:@.arguments/*/prefix
            .:
      .lambda

         // Prefixing prompt.
         set-value:x:@.arguments/*/prompt
            strings.concat
               get-value:x:@.arguments/*/prefix
               get-value:x:@.arguments/*/prompt

   // Adding user's current question to session.
   unwrap:x:+/*/*/*/content
   add:x:@.session
      .
         .
            role:user
            content:x:@.arguments/*/prompt

   // Retrieving token used to invoke OpenAI.
   .token
   set-value:x:@.token
      strings.concat
         .:"Bearer "
         get-first-value
            get-value:x:@.arguments/*/api_key
            config.get:"magic:openai:key"

   // Invoking OpenAI now with context being either training data or previous messages.
   add:x:./*/http.post/*/payload/*/messages
      get-nodes:x:@.session/*
   http.post:"https://api.openai.com/v1/chat/completions"
      headers
         Authorization:x:@.token
         Content-Type:application/json
      payload
         model:x:@.arguments/*/model
         max_tokens:x:@.arguments/*/max_tokens
         temperature:x:@.arguments/*/temperature
         messages
      convert:true

   // Sanity checking above invocation
   if
      not
         and
            mte:x:@http.post
               .:int:200
            lt:x:@http.post
               .:int:300
      .lambda

         // Oops, error - Logging error and returning status 500 to caller.
         lambda2hyper:x:@http.post
         log.error:Something went wrong while invoking OpenAI
            message:x:@http.post/*/content/*/error/*/message
            status:x:@http.post
            error:x:@lambda2hyper
         throw:x:@http.post/*/content/*/error/*/message
            public:bool:true
            status:x:@http.post

   else

      // Success!
      log.info:Invoking OpenAI was a success

   // Making sure we trim response.
   .result
   set-value:x:@.result
      strings.trim:x:@http.post/*/content/*/choices/0/*/message/*/content
   get-first-value
      get-value:x:@http.post/*/content/*/choices/0/*/finish_reason
      .:unknown

   // Adding current prompt to session, but only if caller provided a session.
   if
      and
         exists:x:@.arguments/*/session
         not-null:x:@.arguments/*/session
      .lambda

         // Caller provided a session.
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:system
                  content:x:@.result
         while
            mt:x:@.fixed
               .:int:0
            .lambda
               remove-nodes:x:@.session/0
               math.decrement:x:@.fixed
         lambda2hyper:x:@.session/*
         cache.set:x:@.arguments/*/session
            expiration:600
            value:x:@lambda2hyper

   // Opening database connection to store request into ml_requests table.
   lambda2hyper:x:@.session/*
   log.info:x:-
   data.connect:[generic|magic]

      // Checking if type is "supervised".
      if
         and
            exists:x:@.arguments/*/supervised
            not-null:x:@.arguments/*/supervised
            eq
               convert:x:@.arguments/*/supervised
                  type:int
               .:int:1
         .lambda

            // Storing response to ml_requests to keep history.
            data.create
               table:ml_requests
               values
                  type:x:@.arguments/*/type
                  prompt:x:@.only-prompt
                  completion:x:@.result
                  finish_reason:x:@get-first-value
                  session:x:@.arguments/*/session
                  user_id:x:@.arguments/*/user_id

      // Checking if we've got references.
      if
         and
            exists:x:@.arguments/*/references
            not-null:x:@.arguments/*/references
            get-value:x:@.arguments/*/references
            exists:x:@signal/*/snippets
         .lambda
            add:x:../*/return
               .
                  references
            add:x:../*/return/*/references
               get-nodes:x:@signal/*/snippets/*

   // Checking if we've configured integrations for outgoing messages.
   .outgoing
   set-value:x:@.outgoing
      get-first-value
         get-value:x:@.arguments/*/webhook_outgoing
         config.get:"magic:openai:integrations:outgoing:slot"
   if
      and
         not-null:x:@.outgoing
         neq:x:@.outgoing
            .:
      .lambda

         // Invoking integration slot.
         .exe

            // Retrieving URL to invoke.
            .hook-url
            set-value:x:@.hook-url
               get-first-value
                  get-value:x:@.arguments/*/webhook_outgoing_url
                  config.get:"magic:openai:integrations:outgoing:url"

            // Invoking slot.
            unwrap:x:./*/signal/*/url
            signal:x:@.outgoing
               url:x:@.hook-url

         // Parametrizing invocation to integration slot.
         unwrap:x:+/*/*
         add:x:@.exe/*/signal
            .
               result:x:@.result
               session:x:@.arguments/*/session

         // Parametrizing invocation to integration slot.
         if
            and
               exists:x:@.arguments/*/to
               exists:x:@.arguments/*/from
               not-null:x:@.arguments/*/to
               not-null:x:@.arguments/*/from
               strings.contains:x:@.arguments/*/to
                  .:":"
               strings.contains:x:@.arguments/*/from
                  .:":"
            .lambda

               // We have a channel to accommodate for.
               .channel
               .to
               .from
               strings.split:x:@.arguments/*/to
                  .:":"
               set-value:x:@.channel
                  get-value:x:@strings.split/0
               set-value:x:@.to
                  get-value:x:@strings.split/1
               strings.split:x:@.arguments/*/from
                  .:":"
               set-value:x:@.from
                  get-value:x:@strings.split/1
               unwrap:x:+/*/*
               add:x:@.exe/*/signal
                  .
                     to:x:@.from
                     from:x:@.to
                     channel:x:@.channel

         else

            // No channel
            add:x:@.exe/*/signal
               get-nodes:x:@.arguments/*/to
               get-nodes:x:@.arguments/*/from

         // Invoking callback.
         eval:x:@.exe

   // Returning results returned from invocation above to caller.
   unwrap:x:+/*
   return
      result:x:@.result
      finish_reason:x:@get-first-value
      db_time:x:@signal/*/db_time

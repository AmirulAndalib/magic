
/*
 * Chat slot for having conversations with OpenAI's "gpt" type of models.
 */
slots.create:magic.ai.chat

   // Retrieving relevant snippets.
   unwrap:x:+/*
   signal:magic.ai.get-context
      type:x:@.arguments/*/type
      prompt:x:@.arguments/*/prompt
      threshold:x:@.arguments/*/threshold
      max_tokens:x:@.arguments/*/max_context_tokens
      vector_model:x:@.arguments/*/vector_model
      api_key:x:@.arguments/*/api_key
      search_postfix:x:@.arguments/*/search_postfix

   // Checking if we've got a cached result.
   if
      exists:x:@signal/*/cached
      .lambda

         // Cached result, checking if type is "supervised".
         if
            and
               exists:x:@.arguments/*/supervised
               not-null:x:@.arguments/*/supervised
               eq
                  convert:x:@.arguments/*/supervised
                     type:int
                  .:int:1
            .lambda

               // Storing response to ml_requests to keep history.
               data.connect:[generic|magic]
                  data.create
                     table:ml_requests
                     values
                        type:x:@.arguments/*/type
                        prompt:x:@.arguments/*/prompt
                        completion:x:@signal/*/cached
                        finish_reason:cached
                        session:x:@.arguments/*/session
                        user_id:x:@.arguments/*/user_id

         // Returning cached result to caller.
         unwrap:x:+/*
         return
            result:x:@signal/*/cached
            finish_reason:cached
            stream:bool:false

   /*
    * Session for user containing previous questions and answers,
    * in addition to context and system message.
    */
   .session

   // Checking if we've got a system message.
   if
      and
         not-null:x:@.arguments/*/system_message
         neq:x:@.arguments/*/system_message
            .:
      .lambda

         // Adding system message to [.session]
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:system
                  content:x:@.arguments/*/system_message

   // Then adding context ml_requests belonging to user to [.session]
   data.connect:[generic|magic]

      // Selecting context requests from database.
      data.select:@"
select prompt, max(completion) as completion
   from ml_requests
   where user_id = @user_id and questionnaire = 1 and context = 1
   group by prompt"
         user_id:x:@.arguments/*/user_id

      // Adding context requests to [.session].
      for-each:x:@data.select/*

         // Adding question as system message.
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:assistant
                  content:x:@.dp/#/*/prompt

         // Adding answer as user message.
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:user
                  content:x:@.dp/#/*/completion

   /*
    * Storing how many fixed messages we have to avoid removing
    * system message or questionnaire messages as we're pruning messages
    * to avoid overflowing model.
    */
   .fixed
   set-value:x:@.fixed
      get-count:x:@.session/*

   // Checking if we've got a cached session.
   if
      and
         exists:x:@.arguments/*/session
         not-null:x:@.arguments/*/session
         neq:x:@.arguments/*/session
            .:
      .lambda

         // Caller provided a cache key.
         cache.get:x:@.arguments/*/session
         if
            not-null:x:@cache.get
            .lambda

               // Adding session questions to context.
               add:x:@.session
                  hyper2lambda:x:@cache.get

   // Checking if we've got context.
   if
      not-null:x:@signal/*/context
      .lambda

         // Adding context to above [.session].
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:system
                  content:x:@signal/*/context

   // Checking if we've got a [data] argument, and if so, making sure we accommodate for token size.
   .extra-tokens:int:0
   if
      and
         exists:x:@.arguments/*/data
         not-null:x:@.arguments/*/data
         neq:x:@.arguments/*/data
            .:
      .lambda
         set-value:x:@.extra-tokens
            openai.tokenize:x:@.arguments/*/data

   /*
    * Pruning previous messages until size of context + max_tokens is less than whatever
    * amount of tokens the model can handle.
    */
   .cont:bool:true
   while:x:@.cont

      lambda2hyper:x:@.session/*
      if
         lt
            math.add
               get-value:x:@.arguments/*/max_tokens
               openai.tokenize:x:@lambda2hyper
               get-value:x:@.extra-tokens
            get-value:x:@.arguments/*/model_size
         .lambda

            // Context is small enough to answer according to max_tokens.
            set-value:x:@.cont
               .:bool:false

      else

         /*
          * We need to remove one of our previous messages to be able to have OpenAI return max_tokens.
          * Notice, we only remove messages after our "fixed" messages, which are system message and
          * questionnaire messages.
          */
         strings.concat
            .:@.session/
            get-value:x:@.fixed
         set-x:x:./*/remove-nodes
            convert:x:@strings.concat
               type:x
         remove-nodes

   // Checking if model is prefixed, at which point we prepend prefix to question.
   .only-prompt
   set-value:x:@.only-prompt
      get-value:x:@.arguments/*/prompt
   if
      and
         exists:x:@.arguments/*/prefix
         not-null:x:@.arguments/*/prefix
         neq:x:@.arguments/*/prefix
            .:
      .lambda

         // Prefixing prompt.
         set-value:x:@.arguments/*/prompt
            strings.concat
               get-value:x:@.arguments/*/prefix
               get-value:x:@.arguments/*/prompt

   // Checking if we've got a [data] argument, and if so, adding it.
   if
      and
         exists:x:@.arguments/*/data
         not-null:x:@.arguments/*/data
         neq:x:@.arguments/*/data
            .:
      .lambda
         unwrap:x:+/*/*/*/content
         add:x:@.session
            .
               .
                  role:user
                  content:x:@.arguments/*/data

   // Adding user's current question to session.
   unwrap:x:+/*/*/*/content
   add:x:@.session
      .
         .
            role:user
            content:x:@.arguments/*/prompt

   // Retrieving token used to invoke OpenAI.
   .token
   set-value:x:@.token
      strings.concat
         .:"Bearer "
         get-first-value
            get-value:x:@.arguments/*/api_key
            config.get:"magic:openai:key"

   /*
    * Checking if caller wants to stream tokens.
    *
    * Streaming implies using OpenAI's SSE features, where the
    * invocation returns immediately, for then to invoke our callback
    * as more tokens are available to our client.
    */
   if
      eq:x:@.arguments/*/stream
         .:bool:true
      .lambda

         // We're using streaming or SSE as we're invoking OpenAI.
         unwrap:x:+/*/*/.sse/*/.session
         add:x:../*/.invoke/*/http.post
            .
               .sse

                  /*
                   * Forward evaluated further up, contains the session ID,
                   * and is also our socket channel for notifying client
                   * of new tokens.
                   */
                  .session:x:@.arguments/*/session

                  // Verifying OpenAI sent us actual content.
                  if
                     and
                        exists:x:@.arguments/*/message
                        not-null:x:@.arguments/*/message
                        strings.starts-with:x:@.arguments/*/message
                           .:"data:"
                     .lambda

                        // Parsing message sent to us from OpenAI.
                        strings.substring:x:@.arguments/*/message
                           .:int:5
                        strings.trim:x:@strings.substring

                        // Making sure OpenAI sent us something at all.
                        if
                           neq:x:@strings.trim
                              .:
                           .lambda

                              // Checking if we're done.
                              if
                                 eq:x:@strings.trim
                                    .:[DONE]
                                 .lambda

                                    // OpenAI is done.
                                    sockets.signal:x:@.session
                                       args
                                          finished:bool:true
                              else

                                 // More data to come.
                                 json2lambda:x:@strings.trim

                                 // Updating finish reason if we were given a reason.
                                 if
                                    and
                                       exists:x:@json2lambda/*/choices/0/*/finish_reason
                                       not-null:x:@json2lambda/*/choices/0/*/finish_reason
                                    .lambda

                                       // OpenAI returned a [finish_reason].
                                       set-value:x:@.finish_reason
                                          get-value:x:@json2lambda/*/choices/0/*/finish_reason

                                       // Signaling client
                                       unwrap:x:+/*/*
                                       sockets.signal:x:@.session
                                          args
                                             finish_reason:x:@.finish_reason

                                 // Making sure we have a message.
                                 if
                                    and
                                       not-null:x:@json2lambda/*/choices/0/*/delta/*/content
                                       neq:x:@json2lambda/*/choices/0/*/delta/*/content
                                          .:
                                    .lambda

                                       // Appending content to above [.result].
                                       set-value:x:@.result
                                          strings.concat
                                             get-value:x:@.result
                                             get-value:x:@json2lambda/*/choices/0/*/delta/*/content

                                       // Signaling client with the newly acquired data from OpenAI.
                                       unwrap:x:+/*/*
                                       sockets.signal:x:@.session
                                          args
                                             message:x:@json2lambda/*/choices/0/*/delta/*/content

         /*
          * Making sure we parametrise invocation to OpenAI
          * such that it streams result.
          */
         add:x:../*/.invoke/*/http.post/*/payload
            .
               stream:bool:true

   /*
    * Lambda object actually invoking OpenAI.
    *
    * Notice, if we're streaming the result, this will be invoked one a different thread,
    * otherwise it will be invoked synchronously.
    */
   insert-before:x:./*/.invoke/0
      get-nodes:x:@.arguments
      get-nodes:x:@.session
      get-nodes:x:@.token
      get-nodes:x:@.fixed
      get-nodes:x:@.only-prompt
   .invoke

      // Contains result from OpenAI.
      .result:
      .finish_reason

      // Invoking OpenAI now with context being either training data or previous messages.
      add:x:./*/http.post/*/payload/*/messages
         get-nodes:x:@.session/*
      http.post:"https://api.openai.com/v1/chat/completions"
         convert:bool:true
         headers
            Authorization:x:@.token
            Content-Type:application/json
            Accept:text/event-stream
         payload
            model:x:@.arguments/*/model
            max_tokens:x:@.arguments/*/max_tokens
            temperature:x:@.arguments/*/temperature
            messages

      // Sanity checking above invocation.
      if
         not
            and
               mte:x:@http.post
                  .:int:200
               lt:x:@http.post
                  .:int:300
         .lambda

            // Oops, error - Logging error and returning status 500 to caller.
            lambda2hyper:x:@http.post
            log.error:Something went wrong while invoking OpenAI
               message:x:@http.post/*/content/*/error/*/message
               status:x:@http.post
               error:x:@lambda2hyper

            // We only throw exception if we're NOT streaming.
            if
               or
                  not-exists:x:@.arguments/*/stream
                  eq:x:@.arguments/*/stream
                     .:bool:false
               .lambda

                  // Not streaming, hence we're on the main thread and can safely throw exceptions.
                  throw:x:@http.post/*/content/*/error/*/message
                     public:bool:true
                     status:x:@http.post

            else

               // Signaling client with the error returned from OpenAI.
               unwrap:x:+/*/*
               sockets.signal:x:@.arguments/*/session
                  args
                     error:bool:true
                     status:x:@http.post
                     message:x:@http.post/*/content/*/error/*/message

      else

         // Success!
         log.info:Invoking OpenAI was a success

      /*
       * Checking if we're NOT streaming result, at which point we must
       * set above [.result] to response from HTTP invocation.
       */
      if
         neq:x:@.arguments/*/stream
            .:bool:true
         .lambda

            /*
             * Our HTTP POST invocation will return the actual response,
             * since it's not being streamed using SSE.
             */
            set-value:x:@.result
               get-value:x:@http.post/*/content/*/choices/0/*/message/*/content
            set-value:x:@.finish_reason
               get-first-value
                  get-value:x:@http.post/*/content/*/choices/0/*/finish_reason
                  .:unknown

      // Making sure we trim response.
      set-value:x:@.result
         strings.trim:x:@.result

      // Adding current prompt to session, but only if caller provided a session.
      if
         and
            exists:x:@.arguments/*/session
            not-null:x:@.arguments/*/session
         .lambda

            // Caller provided a session.
            unwrap:x:+/*/*/*/content
            add:x:@.session
               .
                  .
                     role:assistant
                     content:x:@.result
            while
               mt:x:@.fixed
                  .:int:0
               .lambda
                  remove-nodes:x:@.session/0
                  math.decrement:x:@.fixed
            lambda2hyper:x:@.session/*

            // Caching for "session_timeout" seconds (defaults to 20 minutes).
            cache.set:x:@.arguments/*/session
               expiration:x:@.arguments/*/session_timeout
               value:x:@lambda2hyper

      // Checking if type is "supervised" at which point we store request.
      if
         and
            exists:x:@.arguments/*/supervised
            not-null:x:@.arguments/*/supervised
            eq
               convert:x:@.arguments/*/supervised
                  type:int
               .:int:1
         .lambda

            // Opening database connection to store request into ml_requests table.
            data.connect:[generic|magic]

               // Storing response to ml_requests to keep history.
               data.create
                  table:ml_requests
                  values
                     type:x:@.arguments/*/type
                     prompt:x:@.only-prompt
                     completion:x:@.result
                     finish_reason:x:@.finish_reason
                     session:x:@.arguments/*/session
                     user_id:x:@.arguments/*/user_id

      // Checking if we've configured integrations for outgoing messages.
      .outgoing
      set-value:x:@.outgoing
         get-first-value
            get-value:x:@.arguments/*/webhook_outgoing
            config.get:"magic:openai:integrations:outgoing:slot"
      if
         and
            not-null:x:@.outgoing
            neq:x:@.outgoing
               .:
         .lambda

            // Invoking integration slot.
            .exe

               // Retrieving URL to invoke.
               .hook-url
               set-value:x:@.hook-url
                  get-first-value
                     get-value:x:@.arguments/*/webhook_outgoing_url
                     config.get:"magic:openai:integrations:outgoing:url"

               // Invoking slot.
               unwrap:x:./*/signal/*/url
               signal:x:@.outgoing
                  url:x:@.hook-url

            // Parametrizing invocation to integration slot.
            unwrap:x:+/*/*
            add:x:@.exe/*/signal
               .
                  result:x:@.result
                  session:x:@.arguments/*/session

            // Parametrizing invocation to integration slot.
            if
               and
                  exists:x:@.arguments/*/to
                  exists:x:@.arguments/*/from
                  not-null:x:@.arguments/*/to
                  not-null:x:@.arguments/*/from
                  strings.contains:x:@.arguments/*/to
                     .:":"
                  strings.contains:x:@.arguments/*/from
                     .:":"
               .lambda

                  // We have a channel to accommodate for.
                  .channel
                  .to
                  .from
                  strings.split:x:@.arguments/*/to
                     .:":"
                  set-value:x:@.channel
                     get-value:x:@strings.split/0
                  set-value:x:@.to
                     get-value:x:@strings.split/1
                  strings.split:x:@.arguments/*/from
                     .:":"
                  set-value:x:@.from
                     get-value:x:@strings.split/1
                  unwrap:x:+/*/*
                  add:x:@.exe/*/signal
                     .
                        to:x:@.from
                        from:x:@.to
                        channel:x:@.channel

            else

               // No channel
               add:x:@.exe/*/signal
                  get-nodes:x:@.arguments/*/to
                  get-nodes:x:@.arguments/*/from

            // Invoking callback.
            eval:x:@.exe

   /*
    * Checking if we're streaming, at which point we invoke above [.invoke]
    * on a different thread.
    */
   if
      get-value:x:@.arguments/*/stream
      .lambda

         // Invoking [.invoke] on a new thread.
         insert-before:x:./*/fork/0
            get-nodes:x:@.invoke
         fork
            eval:x:@.invoke

   else

      // Invoking above [.invoke] on main thread.
      eval:x:@.invoke

   // Checking if we've got references.
   if
      and
         exists:x:@.arguments/*/references
         not-null:x:@.arguments/*/references
         get-value:x:@.arguments/*/references
         exists:x:@signal/*/snippets
      .lambda

         // We've got references, returning these to caller.
         add:x:../*/return
            .
               references
         add:x:../*/return/*/references
            get-nodes:x:@signal/*/snippets/*

   /*
    * Checking if we're NOT streaming, at which point we return the result
    * of the HTTP invocation to caller.
    */
   if
      neq:x:@.arguments/*/stream
         .:bool:true
      .lambda

         // Not streaming so result was returned by HTTP invocation.
         unwrap:x:+/*/*
         add:x:../*/return
            .
               result:x:@.invoke/*/.result
               finish_reason:x:@.invoke/*/.finish_reason

   // Returning results to caller.
   unwrap:x:+/*
   return
      db_time:x:@signal/*/db_time
      stream:x:@.arguments/*/stream
